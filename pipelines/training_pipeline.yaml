# PIPELINE DEFINITION
# Name: pipeline-xgboost-test
# Description: An example pipeline.
# Outputs:
#    train-metrics: system.Metrics
components:
  comp-engineer-features:
    executorLabel: exec-engineer-features
    inputDefinitions:
      artifacts:
        prep_file:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
          description: the preprocessed file
    outputDefinitions:
      artifacts:
        feat_file:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-preprocess:
    executorLabel: exec-preprocess
    inputDefinitions:
      parameters:
        raw_file:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        prep_file:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train:
    executorLabel: exec-train
    inputDefinitions:
      artifacts:
        feat_file:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        train_size:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-engineer-features:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - engineer_features
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef engineer_features(\n    prep_file: Input[Dataset],\n    feat_file:\
          \ Output[Dataset],\n) -> None:\n    \"\"\"\n    Create dummy features\n\n\
          \    Args:\n        prep_file: the preprocessed file\n\n    Return:\n  \
          \      feat_train_file: the features splitted by train\n        feat_test_file:\
          \ the features splitted by test\n        feat_valid_file: the features splitted\
          \ by valid\n    \"\"\"\n    import pandas as pd\n\n    df_prep = pd.read_csv(prep_file.path)\n\
          \n    # Create dummy features\n    df_prep[\"cap_1_cap_2\"] = df_prep[\"\
          cap_1\"] * df_prep[\"cap_2\"]\n    df_prep[\"cap_4_cap_2\"] = df_prep[\"\
          cap_4\"] * df_prep[\"cap_2\"]\n    df_prep[\"cap_3_cap_1\"] = df_prep[\"\
          cap_3\"] * df_prep[\"cap_1\"]\n\n    # Removing string columns\n    df_prep\
          \ = df_prep.drop(columns=[\"country_city\", \"country\", \"city\"])\n\n\
          \    # Rename columns\n    df_prep = df_prep.rename(columns={\"points\"\
          : \"target\"})\n\n    df_prep.to_csv(feat_file.path, index=False)\n\n"
        image: index.docker.io/seyco/base_kbf_image
    exec-preprocess:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess(\n    raw_file: str,\n    prep_file: Output[Dataset],\n\
          ) -> None:\n    \"\"\"\n    Preprocess raw file\n\n    Args:\n        raw_file:\
          \ the file that gathered all the raw data\n        prep_file: the output\
          \ file that contains basic preprocessing\n        steps\n\n    Return:\n\
          \        None\n    \"\"\"\n    import pandas as pd\n\n    df_input = pd.DataFrame({'points':\
          \ [0, 1, 0, 0, 0, 1, 1, 1, 0, 0], 'country_city': ['peru,comas', 'peru,lince',\
          \ 'peru,los olivos', 'peru,san juan de lurigancho', 'peru,comas', 'peru,lince',\
          \ 'peru,los olivos', 'peru,san juan de lurigancho', 'peru,comas', 'peru,lince'],\
          \ 'cap_1': [0.639, 0.109, 0.63, 0.026, 0.53, 0.889, 0.874, 0.196, 0.891,\
          \ 0.915], 'cap_2': [0.466, 0.394, 0.12, 0.571, 0.225, 0.681, 0.459, 0.832,\
          \ 0.019, 0.975], 'cap_3': [0.867, 0.965, 0.961, 0.249, 0.993, 0.218, 0.4,\
          \ 0.916, 0.516, 0.855], 'cap_4': [0.983, 0.658, 0.351, 0.551, 0.451, 0.007,\
          \ 0.6, 0.055, 0.73, 0.017], 'cap_5': [0.689, 0.333, 0.63, 0.772, 0.389,\
          \ 0.885, 0.311, 0.03, 0.253, 0.164], 'cap_6': [0.673, 0.079, 0.987, 0.404,\
          \ 0.357, 0.55, 0.981, 0.641, 0.042, 0.894]})\n    df_input[[\"country\"\
          , \"city\"]] = df_input[\"country_city\"].apply(lambda x: pd.Series(x.split(\"\
          ,\")))\n    df_input.to_csv(prep_file.path, index=False)\n\n"
        image: index.docker.io/seyco/base_kbf_image
    exec-train:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'kfp==2.0.0'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train(\n    feat_file: Input[Dataset],\n    metrics: Output[Metrics],\n\
          \    model: Output[Model],\n    train_size: float = 0.80,\n):\n    \"\"\"\
          \n    Train an XGBoost classifier\n\n    Args:\n        train_file: the\
          \ dataset that contains the train data\n        test_file: the dataset that\
          \ contains the test data\n        valid_file: the dataset that contains\
          \ the valid data\n    Return:\n\n    \"\"\"\n    import xgboost\n    import\
          \ pickle\n    import pandas as pd\n    from sklearn.model_selection import\
          \ train_test_split\n    from sklearn.metrics import roc_auc_score\n\n  \
          \  if train_size < 0 and train_size > 0.8:\n        raise Exception(\"Invalid\
          \ train size... should be less than 0.8 and greater than 0\") \n\n    df_data\
          \ = pd.read_csv(feat_file.path)\n\n    x_rest, x_test, y_rest, y_test =\
          \ train_test_split(df_data.drop(columns=\"target\"), df_data[[\"target\"\
          ]], test_size=0.2)\n    x_train, x_val, y_train, y_val = train_test_split(x_rest,\
          \ y_rest, test_size=0.1)\n\n    params = {\n        \"n_estimators\": 2,\
          \ \n        \"max_depth\": 3, \n        \"learning_rate\": 1, \n       \
          \ \"objective\": \"binary:logistic\",\n    }\n    bst = xgboost.XGBClassifier(**params,\
          \ random_state=0)\n    # fit model\n    bst.fit(x_train, y_train, eval_set=[(x_val,\
          \ y_val)])\n    # make predictions\n    preds = bst.predict_proba(x_test)[:,\
          \ 1]\n\n    metrics.log_metric(\"accuracy\",roc_auc_score(y_test, preds))\n\
          \    metrics.log_metric(\"model\", \"XGBoost\")\n    metrics.log_metric(\"\
          dataset_size\", len(x_train))\n\n    with open(model.path, \"wb\") as model_path:\n\
          \        pickle.dump(bst, model_path)\n\n"
        image: index.docker.io/seyco/base_kbf_image
pipelineInfo:
  description: An example pipeline.
  name: pipeline-xgboost-test
root:
  dag:
    outputs:
      artifacts:
        train-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: train
    tasks:
      engineer-features:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-engineer-features
        dependentTasks:
        - preprocess
        inputs:
          artifacts:
            prep_file:
              taskOutputArtifact:
                outputArtifactKey: prep_file
                producerTask: preprocess
        taskInfo:
          name: engineer-features
      preprocess:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-preprocess
        inputs:
          parameters:
            raw_file:
              runtimeValue:
                constant: people.csv
        taskInfo:
          name: preprocess
      train:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train
        dependentTasks:
        - engineer-features
        inputs:
          artifacts:
            feat_file:
              taskOutputArtifact:
                outputArtifactKey: feat_file
                producerTask: engineer-features
        taskInfo:
          name: train
  outputDefinitions:
    artifacts:
      train-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0
